cuda memory allocated: 499900928
> n_trainable_params: 124649477, n_nontrainable_params: 0
> training arguments:
>>> model_name: ROBERTA
>>> dataset: small
>>> optimizer: <class 'torch.optim.adamw.AdamW'>
>>> initializer: <function kaiming_uniform_ at 0x0000024A71A3A4D0>
>>> lr: 2e-05
>>> dropout: 0.1
>>> l2reg: 1e-05
>>> epochs: 5
>>> batch_size: 32
>>> log_step: 10
>>> max_seq_len: 512
>>> num_classes: 5
>>> device: cuda
>>> seed: 42
>>> valset_ratio: 0.2
>>> model_class: <class 'models.roberta.ROBERTA'>
>>> dataset_file: {'train': 'dataset/train_small.csv', 'test': 'dataset/test.csv'}
>>> bert_config_path: ../../../pretrained/BertConfig/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594
>>> bert_model_path: ../../../pretrained/BertModel/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b
>>> bert_tokenizer_path: ../../../pretrained/BertTokenizer/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b
>>> roberta_config_path: ../../../pretrained/RobertaConfig/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b
>>> roberta_model_path: ../../../pretrained/RobertaModel/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b
>>> roberta_tokenizer_path: ../../../pretrained/RobertaTokenizer/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b
>>> tokenizer: RobertaTokenizer(name_or_path='../../../pretrained/RobertaTokenizer/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b', vocab_size=50265, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={
	0: AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
	1: AddedToken("<pad>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
	3: AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
	50264: AddedToken("<mask>", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),
}
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.5373, acc: 0.3125
loss: 1.4198, acc: 0.4109
loss: 1.3436, acc: 0.4521
loss: 1.2737, acc: 0.4844
loss: 1.2462, acc: 0.4938
loss: 1.2185, acc: 0.5005
loss: 1.1771, acc: 0.5156
loss: 1.1474, acc: 0.5301
loss: 1.1291, acc: 0.5372
loss: 1.1296, acc: 0.5403
loss: 1.1146, acc: 0.5514
loss: 1.0970, acc: 0.5586
loss: 1.0761, acc: 0.5695
loss: 1.0679, acc: 0.5694
loss: 1.0555, acc: 0.5740
loss: 1.0430, acc: 0.5787
loss: 1.0334, acc: 0.5809
loss: 1.0211, acc: 0.5859
loss: 1.0122, acc: 0.5885
loss: 1.0088, acc: 0.5894
loss: 1.0046, acc: 0.5938
loss: 0.9987, acc: 0.5974
loss: 0.9944, acc: 0.5997
loss: 0.9894, acc: 0.6012
loss: 0.9844, acc: 0.6021
loss: 0.9788, acc: 0.6044
loss: 0.9715, acc: 0.6074
loss: 0.9647, acc: 0.6114
loss: 0.9580, acc: 0.6140
loss: 0.9539, acc: 0.6154
loss: 0.9509, acc: 0.6158
loss: 0.9485, acc: 0.6162
loss: 0.9459, acc: 0.6180
loss: 0.9427, acc: 0.6195
loss: 0.9405, acc: 0.6197
loss: 0.9364, acc: 0.6214
loss: 0.9319, acc: 0.6234
loss: 0.9269, acc: 0.6248
loss: 0.9237, acc: 0.6269
> val_acc: 0.6825, val_f1: 0.5542
>> saved: state_dict/ROBERTA_small_val_acc_0.6825
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.7744, acc: 0.7118
loss: 0.7297, acc: 0.7220
loss: 0.7312, acc: 0.7177
loss: 0.7312, acc: 0.7051
loss: 0.7226, acc: 0.7117
loss: 0.7129, acc: 0.7150
loss: 0.7151, acc: 0.7106
loss: 0.7159, acc: 0.7104
loss: 0.7188, acc: 0.7075
loss: 0.7228, acc: 0.7045
loss: 0.7221, acc: 0.7021
loss: 0.7221, acc: 0.7017
loss: 0.7208, acc: 0.7030
loss: 0.7166, acc: 0.7066
loss: 0.7184, acc: 0.7045
loss: 0.7179, acc: 0.7038
loss: 0.7152, acc: 0.7064
loss: 0.7164, acc: 0.7048
loss: 0.7166, acc: 0.7049
loss: 0.7172, acc: 0.7057
loss: 0.7199, acc: 0.7056
loss: 0.7231, acc: 0.7036
loss: 0.7241, acc: 0.7032
loss: 0.7263, acc: 0.7018
loss: 0.7269, acc: 0.7016
loss: 0.7260, acc: 0.7020
loss: 0.7233, acc: 0.7031
loss: 0.7247, acc: 0.7019
loss: 0.7260, acc: 0.7017
loss: 0.7244, acc: 0.7030
loss: 0.7245, acc: 0.7038
loss: 0.7242, acc: 0.7046
loss: 0.7221, acc: 0.7056
loss: 0.7221, acc: 0.7052
loss: 0.7199, acc: 0.7055
loss: 0.7216, acc: 0.7046
loss: 0.7209, acc: 0.7048
loss: 0.7192, acc: 0.7056
loss: 0.7178, acc: 0.7063
> val_acc: 0.6889, val_f1: 0.5742
>> saved: state_dict/ROBERTA_small_val_acc_0.6889
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.5740, acc: 0.7734
loss: 0.5802, acc: 0.7569
loss: 0.5920, acc: 0.7578
loss: 0.6142, acc: 0.7467
loss: 0.6084, acc: 0.7461
loss: 0.6076, acc: 0.7473
loss: 0.6232, acc: 0.7403
loss: 0.6152, acc: 0.7452
loss: 0.6117, acc: 0.7496
loss: 0.6135, acc: 0.7513
loss: 0.6184, acc: 0.7465
loss: 0.6209, acc: 0.7468
loss: 0.6266, acc: 0.7454
loss: 0.6273, acc: 0.7466
loss: 0.6293, acc: 0.7458
loss: 0.6296, acc: 0.7466
loss: 0.6275, acc: 0.7474
loss: 0.6251, acc: 0.7472
loss: 0.6222, acc: 0.7482
loss: 0.6261, acc: 0.7449
loss: 0.6278, acc: 0.7429
loss: 0.6273, acc: 0.7425
loss: 0.6282, acc: 0.7426
loss: 0.6297, acc: 0.7421
loss: 0.6326, acc: 0.7403
loss: 0.6321, acc: 0.7409
loss: 0.6330, acc: 0.7410
loss: 0.6360, acc: 0.7403
loss: 0.6368, acc: 0.7397
loss: 0.6384, acc: 0.7396
loss: 0.6377, acc: 0.7407
loss: 0.6391, acc: 0.7394
loss: 0.6406, acc: 0.7383
loss: 0.6404, acc: 0.7377
loss: 0.6405, acc: 0.7373
loss: 0.6412, acc: 0.7374
loss: 0.6416, acc: 0.7377
loss: 0.6407, acc: 0.7388
loss: 0.6387, acc: 0.7393
> val_acc: 0.6838, val_f1: 0.5825
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.5189, acc: 0.7902
loss: 0.5405, acc: 0.7721
loss: 0.5289, acc: 0.7824
loss: 0.5455, acc: 0.7736
loss: 0.5409, acc: 0.7812
loss: 0.5398, acc: 0.7818
loss: 0.5326, acc: 0.7840
loss: 0.5352, acc: 0.7825
loss: 0.5379, acc: 0.7805
loss: 0.5486, acc: 0.7742
loss: 0.5406, acc: 0.7786
loss: 0.5426, acc: 0.7748
loss: 0.5468, acc: 0.7741
loss: 0.5496, acc: 0.7721
loss: 0.5446, acc: 0.7723
loss: 0.5402, acc: 0.7739
loss: 0.5385, acc: 0.7745
loss: 0.5386, acc: 0.7749
loss: 0.5400, acc: 0.7754
loss: 0.5401, acc: 0.7763
loss: 0.5414, acc: 0.7761
loss: 0.5437, acc: 0.7755
loss: 0.5457, acc: 0.7752
loss: 0.5459, acc: 0.7744
loss: 0.5444, acc: 0.7744
loss: 0.5465, acc: 0.7738
loss: 0.5492, acc: 0.7722
loss: 0.5518, acc: 0.7718
loss: 0.5546, acc: 0.7705
loss: 0.5564, acc: 0.7701
loss: 0.5554, acc: 0.7704
loss: 0.5553, acc: 0.7696
loss: 0.5548, acc: 0.7697
loss: 0.5554, acc: 0.7688
loss: 0.5564, acc: 0.7687
loss: 0.5553, acc: 0.7689
loss: 0.5565, acc: 0.7686
loss: 0.5586, acc: 0.7673
loss: 0.5593, acc: 0.7675
> val_acc: 0.6831, val_f1: 0.5708
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.5183, acc: 0.7917
loss: 0.4511, acc: 0.8125
loss: 0.4450, acc: 0.8113
loss: 0.4472, acc: 0.8142
loss: 0.4539, acc: 0.8125
loss: 0.4636, acc: 0.8092
loss: 0.4713, acc: 0.8063
loss: 0.4632, acc: 0.8117
loss: 0.4589, acc: 0.8125
loss: 0.4584, acc: 0.8115
loss: 0.4617, acc: 0.8119
loss: 0.4634, acc: 0.8106
loss: 0.4626, acc: 0.8127
loss: 0.4641, acc: 0.8123
loss: 0.4669, acc: 0.8084
loss: 0.4704, acc: 0.8061
loss: 0.4730, acc: 0.8042
loss: 0.4769, acc: 0.8022
loss: 0.4804, acc: 0.8004
loss: 0.4815, acc: 0.7996
loss: 0.4812, acc: 0.7998
loss: 0.4817, acc: 0.8002
loss: 0.4866, acc: 0.7983
loss: 0.4864, acc: 0.7990
loss: 0.4885, acc: 0.7978
loss: 0.4910, acc: 0.7975
loss: 0.4935, acc: 0.7965
loss: 0.4953, acc: 0.7960
loss: 0.4975, acc: 0.7944
loss: 0.4971, acc: 0.7937
loss: 0.4999, acc: 0.7919
loss: 0.5022, acc: 0.7906
loss: 0.5003, acc: 0.7911
loss: 0.4989, acc: 0.7917
loss: 0.4981, acc: 0.7916
loss: 0.4987, acc: 0.7906
loss: 0.4986, acc: 0.7908
loss: 0.5000, acc: 0.7901
loss: 0.5003, acc: 0.7902
> val_acc: 0.6668, val_f1: 0.5647
