cuda memory allocated: 244567040
> n_trainable_params: 59541504, n_nontrainable_params: 0
> training arguments:
>>> model_name: GPT2-Chinese
>>> dataset: raw
>>> optimizer: <class 'torch.optim.adamw.AdamW'>
>>> initializer: <function kaiming_uniform_ at 0x0000021F20DA5BD0>
>>> lr: 0.001
>>> dropout: 0.1
>>> l2reg: 0.01
>>> epochs: 50
>>> batch_size: 16
>>> log_step: 1
>>> max_seq_len: 128
>>> device: cuda
>>> seed: 42
>>> valset_ratio: 0.0
>>> gpt2_name: uer/gpt2-distil-chinese-cluecorpussmall
>>> model_class: <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>
>>> dataset_file: {'train': 'dataset/poetryFromTang.txt'}
>>> gpt2_config_path: ../../../pretrained/GPT2Config/models--uer--gpt2-distil-chinese-cluecorpussmall/snapshots/c98ef629a1ece266e9d9183add4cbe5d4b99c7d5
>>> gpt2_tokenizer_path: ../../../pretrained/BertTokenizer/models--uer--gpt2-distil-chinese-cluecorpussmall/snapshots/c98ef629a1ece266e9d9183add4cbe5d4b99c7d5
>>> gpt2_model_path: ../../../pretrained/GPT2Model/models--uer--gpt2-distil-chinese-cluecorpussmall/snapshots/c98ef629a1ece266e9d9183add4cbe5d4b99c7d5
>>> tokenizer: BertTokenizer(name_or_path='../../../pretrained/BertTokenizer/models--uer--gpt2-distil-chinese-cluecorpussmall/snapshots/c98ef629a1ece266e9d9183add4cbe5d4b99c7d5', vocab_size=21128, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={
	0: AddedToken("[PAD]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	100: AddedToken("[UNK]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	101: AddedToken("[CLS]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	102: AddedToken("[SEP]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	103: AddedToken("[MASK]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 9.9913, perplexity: 21835.5129
loss: 9.7856, perplexity: 17775.9329
loss: 9.7004, perplexity: 16324.1137
loss: 9.6317, perplexity: 15240.1605
loss: 9.5785, perplexity: 14450.1580
loss: 9.5432, perplexity: 13948.9455
loss: 9.5045, perplexity: 13419.9987
loss: 9.4610, perplexity: 12848.3467
loss: 9.4287, perplexity: 12440.4456
loss: 9.3938, perplexity: 12014.2237
loss: 9.3929, perplexity: 12003.0451
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 8.7496, perplexity: 6308.0045
loss: 8.7179, perplexity: 6111.4806
loss: 8.6770, perplexity: 5866.5071
loss: 8.6262, perplexity: 5575.6761
loss: 8.5844, perplexity: 5347.3782
loss: 8.5678, perplexity: 5259.6364
loss: 8.5115, perplexity: 4971.4304
loss: 8.4515, perplexity: 4682.2638
loss: 8.3855, perplexity: 4383.2079
loss: 8.3406, perplexity: 4190.4508
loss: 8.3236, perplexity: 4119.8355
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 7.5756, perplexity: 1950.0367
loss: 7.4200, perplexity: 1668.9685
loss: 7.4292, perplexity: 1684.4402
loss: 7.3071, perplexity: 1490.8266
loss: 7.2146, perplexity: 1359.0933
loss: 7.1584, perplexity: 1284.9138
loss: 7.0732, perplexity: 1179.8674
loss: 6.9901, perplexity: 1085.8110
loss: 6.9127, perplexity: 1005.0027
loss: 6.8320, perplexity: 927.0414
loss: 6.8090, perplexity: 905.9267
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 5.5494, perplexity: 257.0787
loss: 5.8098, perplexity: 333.5406
loss: 5.6980, perplexity: 298.2663
loss: 5.6284, perplexity: 278.2159
loss: 5.5031, perplexity: 245.4441
loss: 5.4523, perplexity: 233.3040
loss: 5.3376, perplexity: 208.0160
loss: 5.3367, perplexity: 207.8212
loss: 5.2526, perplexity: 191.0674
loss: 5.1997, perplexity: 181.2246
loss: 5.2119, perplexity: 183.4377
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 4.5830, perplexity: 97.8101
loss: 4.3438, perplexity: 76.9962
loss: 4.2176, perplexity: 67.8721
loss: 4.3879, perplexity: 80.4728
loss: 4.3079, perplexity: 74.2849
loss: 4.2792, perplexity: 72.1832
loss: 4.1896, perplexity: 65.9996
loss: 4.1292, perplexity: 62.1271
loss: 4.1079, perplexity: 60.8192
loss: 4.0915, perplexity: 59.8323
loss: 4.1230, perplexity: 61.7439
>> saved: state_dict/GPT2-Chinese_raw_test_perplexity_61.7439
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 3.7533, perplexity: 42.6635
loss: 3.8640, perplexity: 47.6555
loss: 3.8867, perplexity: 48.7492
loss: 3.7692, perplexity: 43.3450
loss: 3.8187, perplexity: 45.5441
loss: 3.7852, perplexity: 44.0435
loss: 3.7318, perplexity: 41.7529
loss: 3.6735, perplexity: 39.3883
loss: 3.7770, perplexity: 43.6864
loss: 3.7076, perplexity: 40.7572
loss: 3.7018, perplexity: 40.5186
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 3.7709, perplexity: 43.4188
loss: 3.6652, perplexity: 39.0635
loss: 3.4601, perplexity: 31.8210
loss: 3.4896, perplexity: 32.7719
loss: 3.5265, perplexity: 34.0032
loss: 3.5652, perplexity: 35.3480
loss: 3.5372, perplexity: 34.3705
loss: 3.5172, perplexity: 33.6890
loss: 3.5709, perplexity: 35.5498
loss: 3.5323, perplexity: 34.2025
loss: 3.6026, perplexity: 36.6925
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 3.4890, perplexity: 32.7517
loss: 3.6752, perplexity: 39.4549
loss: 3.5525, perplexity: 34.9016
loss: 3.4930, perplexity: 32.8848
loss: 3.5390, perplexity: 34.4326
loss: 3.5421, perplexity: 34.5383
loss: 3.5446, perplexity: 34.6272
loss: 3.4563, perplexity: 31.7009
loss: 3.5549, perplexity: 34.9844
loss: 3.5123, perplexity: 33.5256
loss: 3.5250, perplexity: 33.9526
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 3.6966, perplexity: 40.3100
loss: 3.4873, perplexity: 32.6979
loss: 3.6504, perplexity: 38.4905
loss: 3.5353, perplexity: 34.3059
loss: 3.4642, perplexity: 31.9521
loss: 3.5599, perplexity: 35.1594
loss: 3.4594, perplexity: 31.7966
loss: 3.5363, perplexity: 34.3394
loss: 3.5054, perplexity: 33.2938
loss: 3.4633, perplexity: 31.9227
loss: 3.4572, perplexity: 31.7292
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
loss: 3.4498, perplexity: 31.4945
loss: 3.1437, perplexity: 23.1889
loss: 3.4124, perplexity: 30.3383
loss: 3.2305, perplexity: 25.2929
loss: 3.3627, perplexity: 28.8669
loss: 3.5729, perplexity: 35.6185
loss: 3.5888, perplexity: 36.1913
loss: 3.5127, perplexity: 33.5387
loss: 3.4271, perplexity: 30.7885
loss: 3.4117, perplexity: 30.3179
loss: 3.4710, perplexity: 32.1680
>> saved: state_dict/GPT2-Chinese_raw_test_perplexity_32.168
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 10
loss: 4.0395, perplexity: 56.7995
loss: 3.6795, perplexity: 39.6259
loss: 3.5237, perplexity: 33.9082
loss: 3.3026, perplexity: 27.1826
loss: 3.4571, perplexity: 31.7248
loss: 3.4579, perplexity: 31.7497
loss: 3.4881, perplexity: 32.7238
loss: 3.4321, perplexity: 30.9415
loss: 3.4364, perplexity: 31.0762
loss: 3.3773, perplexity: 29.2904
loss: 3.3914, perplexity: 29.7084
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 11
loss: 3.2134, perplexity: 24.8631
loss: 3.1983, perplexity: 24.4907
loss: 3.3388, perplexity: 28.1866
loss: 3.5589, perplexity: 35.1234
loss: 3.5273, perplexity: 34.0322
loss: 3.4349, perplexity: 31.0272
loss: 3.3973, perplexity: 29.8823
loss: 3.4049, perplexity: 30.1109
loss: 3.3949, perplexity: 29.8120
loss: 3.3402, perplexity: 28.2260
loss: 3.3678, perplexity: 29.0147
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 12
loss: 3.1673, perplexity: 23.7436
loss: 2.9931, perplexity: 19.9468
loss: 3.2865, perplexity: 26.7501
loss: 3.4423, perplexity: 31.2602
loss: 3.3959, perplexity: 29.8417
loss: 3.5471, perplexity: 34.7141
loss: 3.5832, perplexity: 35.9871
loss: 3.5555, perplexity: 35.0053
loss: 3.4614, perplexity: 31.8603
loss: 3.4310, perplexity: 30.9078
loss: 3.4771, perplexity: 32.3660
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 13
loss: 2.9592, perplexity: 19.2826
loss: 2.9934, perplexity: 19.9538
loss: 3.1759, perplexity: 23.9476
loss: 3.3030, perplexity: 27.1941
loss: 3.2323, perplexity: 25.3372
loss: 3.2940, perplexity: 26.9504
loss: 3.2935, perplexity: 26.9357
loss: 3.3062, perplexity: 27.2818
loss: 3.2672, perplexity: 26.2390
loss: 3.2449, perplexity: 25.6602
loss: 3.2474, perplexity: 25.7230
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 14
loss: 3.1744, perplexity: 23.9114
loss: 3.0765, perplexity: 21.6826
loss: 3.2786, perplexity: 26.5384
loss: 3.1023, perplexity: 22.2487
loss: 3.1639, perplexity: 23.6617
loss: 3.1653, perplexity: 23.6968
loss: 3.3006, perplexity: 27.1291
loss: 3.3657, perplexity: 28.9539
loss: 3.3066, perplexity: 27.2919
loss: 3.2570, perplexity: 25.9711
loss: 3.2662, perplexity: 26.2114
>> saved: state_dict/GPT2-Chinese_raw_test_perplexity_26.2114
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 15
loss: 3.8873, perplexity: 48.7786
loss: 3.8055, perplexity: 44.9489
loss: 3.6748, perplexity: 39.4416
loss: 3.4470, perplexity: 31.4070
loss: 3.3277, perplexity: 27.8736
loss: 3.2443, perplexity: 25.6447
loss: 3.2005, perplexity: 24.5442
loss: 3.2850, perplexity: 26.7096
loss: 3.2409, perplexity: 25.5559
loss: 3.2440, perplexity: 25.6371
loss: 3.2348, perplexity: 25.4010
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 16
loss: 3.0095, perplexity: 20.2773
loss: 3.4681, perplexity: 32.0751
loss: 3.3165, perplexity: 27.5643
loss: 3.2530, perplexity: 25.8670
loss: 3.3556, perplexity: 28.6630
loss: 3.4043, perplexity: 30.0942
loss: 3.3257, perplexity: 27.8178
loss: 3.2409, perplexity: 25.5558
loss: 3.2741, perplexity: 26.4181
loss: 3.2043, perplexity: 24.6380
loss: 3.1997, perplexity: 24.5251
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 17
loss: 3.4633, perplexity: 31.9227
loss: 3.4663, perplexity: 32.0168
loss: 3.3611, perplexity: 28.8218
loss: 3.2391, perplexity: 25.5113
loss: 3.1283, perplexity: 22.8358
loss: 3.0587, perplexity: 21.3008
loss: 2.9806, perplexity: 19.6995
loss: 3.0363, perplexity: 20.8287
loss: 3.1466, perplexity: 23.2578
loss: 3.1410, perplexity: 23.1268
loss: 3.1398, perplexity: 23.0985
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 18
loss: 2.7723, perplexity: 15.9955
loss: 2.9262, perplexity: 18.6573
loss: 3.0627, perplexity: 21.3859
loss: 3.0354, perplexity: 20.8102
loss: 3.0013, perplexity: 20.1116
loss: 2.9242, perplexity: 18.6188
loss: 3.0357, perplexity: 20.8155
loss: 3.0686, perplexity: 21.5110
loss: 3.0346, perplexity: 20.7929
loss: 3.0606, perplexity: 21.3404
loss: 3.0805, perplexity: 21.7699
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 19
loss: 3.1480, perplexity: 23.2891
loss: 3.1878, perplexity: 24.2344
loss: 3.1532, perplexity: 23.4105
loss: 3.0363, perplexity: 20.8285
loss: 3.0085, perplexity: 20.2561
loss: 3.0268, perplexity: 20.6304
loss: 3.0217, perplexity: 20.5261
loss: 3.0129, perplexity: 20.3457
loss: 2.9898, perplexity: 19.8826
loss: 2.9922, perplexity: 19.9294
loss: 3.0070, perplexity: 20.2272
>> saved: state_dict/GPT2-Chinese_raw_test_perplexity_20.2272
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 20
loss: 2.2827, perplexity: 9.8030
loss: 2.6456, perplexity: 14.0917
loss: 2.8542, perplexity: 17.3598
loss: 2.9964, perplexity: 20.0141
loss: 2.9340, perplexity: 18.8018
loss: 3.0252, perplexity: 20.5981
loss: 3.0223, perplexity: 20.5384
loss: 3.0069, perplexity: 20.2249
loss: 3.0250, perplexity: 20.5942
loss: 2.9564, perplexity: 19.2288
loss: 2.9575, perplexity: 19.2496
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 21
loss: 2.5740, perplexity: 13.1184
loss: 2.5167, perplexity: 12.3876
loss: 2.6630, perplexity: 14.3399
loss: 2.8416, perplexity: 17.1428
loss: 2.8096, perplexity: 16.6026
loss: 2.8656, perplexity: 17.5601
loss: 2.9324, perplexity: 18.7728
loss: 2.9119, perplexity: 18.3920
loss: 2.8530, perplexity: 17.3391
loss: 2.8334, perplexity: 17.0027
loss: 2.8575, perplexity: 17.4181
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 22
loss: 2.8434, perplexity: 17.1748
loss: 2.9084, perplexity: 18.3279
loss: 2.9143, perplexity: 18.4356
loss: 2.7858, perplexity: 16.2127
loss: 2.7269, perplexity: 15.2859
loss: 2.7575, perplexity: 15.7610
loss: 2.7059, perplexity: 14.9678
loss: 2.7210, perplexity: 15.1951
loss: 2.7883, perplexity: 16.2531
loss: 2.7873, perplexity: 16.2374
loss: 2.8020, perplexity: 16.4774
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 23
loss: 2.6402, perplexity: 14.0155
loss: 2.4557, perplexity: 11.6542
loss: 2.4371, perplexity: 11.4397
loss: 2.7535, perplexity: 15.6982
loss: 2.7287, perplexity: 15.3124
loss: 2.7193, perplexity: 15.1692
loss: 2.6845, perplexity: 14.6503
loss: 2.6885, perplexity: 14.7101
loss: 2.6420, perplexity: 14.0416
loss: 2.6206, perplexity: 13.7442
loss: 2.6754, perplexity: 14.5177
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 24
loss: 2.1538, perplexity: 8.6176
loss: 2.3535, perplexity: 10.5219
loss: 2.5892, perplexity: 13.3193
loss: 2.5217, perplexity: 12.4501
loss: 2.5143, perplexity: 12.3574
loss: 2.4858, perplexity: 12.0105
loss: 2.4475, perplexity: 11.5598
loss: 2.4557, perplexity: 11.6542
loss: 2.5718, perplexity: 13.0897
loss: 2.5455, perplexity: 12.7498
loss: 2.5422, perplexity: 12.7077
>> saved: state_dict/GPT2-Chinese_raw_test_perplexity_12.7077
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 25
loss: 2.2202, perplexity: 9.2088
loss: 2.2477, perplexity: 9.4659
loss: 2.4760, perplexity: 11.8938
loss: 2.4332, perplexity: 11.3957
loss: 2.4286, perplexity: 11.3425
loss: 2.4133, perplexity: 11.1704
loss: 2.3603, perplexity: 10.5944
loss: 2.3517, perplexity: 10.5031
loss: 2.3566, perplexity: 10.5552
loss: 2.4072, perplexity: 11.1029
loss: 2.4195, perplexity: 11.2406
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 26
loss: 2.0928, perplexity: 8.1073
loss: 2.3578, perplexity: 10.5680
loss: 2.4459, perplexity: 11.5411
loss: 2.3509, perplexity: 10.4952
loss: 2.3171, perplexity: 10.1467
loss: 2.2892, perplexity: 9.8666
loss: 2.2583, perplexity: 9.5671
loss: 2.3034, perplexity: 10.0082
loss: 2.2580, perplexity: 9.5640
loss: 2.2590, perplexity: 9.5736
loss: 2.2753, perplexity: 9.7309
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 27
loss: 1.7540, perplexity: 5.7778
loss: 1.8641, perplexity: 6.4503
loss: 2.0038, perplexity: 7.4174
loss: 1.9840, perplexity: 7.2716
loss: 2.0688, perplexity: 7.9151
loss: 2.0401, perplexity: 7.6917
loss: 2.0505, perplexity: 7.7715
loss: 2.1446, perplexity: 8.5385
loss: 2.1298, perplexity: 8.4134
loss: 2.1392, perplexity: 8.4925
loss: 2.1314, perplexity: 8.4268
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 28
loss: 1.9175, perplexity: 6.8042
loss: 2.0008, perplexity: 7.3946
loss: 1.8887, perplexity: 6.6109
loss: 1.8533, perplexity: 6.3811
loss: 1.9853, perplexity: 7.2810
loss: 1.9237, perplexity: 6.8461
loss: 1.9038, perplexity: 6.7115
loss: 2.0003, perplexity: 7.3911
loss: 1.9765, perplexity: 7.2176
loss: 1.9889, perplexity: 7.3077
loss: 1.9855, perplexity: 7.2824
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 29
loss: 1.7813, perplexity: 5.9376
loss: 1.6368, perplexity: 5.1387
loss: 1.7512, perplexity: 5.7615
loss: 1.7436, perplexity: 5.7181
loss: 1.6805, perplexity: 5.3680
loss: 1.7307, perplexity: 5.6446
loss: 1.8499, perplexity: 6.3591
loss: 1.8799, perplexity: 6.5528
loss: 1.8622, perplexity: 6.4380
loss: 1.8276, perplexity: 6.2192
loss: 1.8429, perplexity: 6.3150
>> saved: state_dict/GPT2-Chinese_raw_test_perplexity_6.315
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 30
loss: 1.5375, perplexity: 4.6530
loss: 1.4848, perplexity: 4.4143
loss: 1.5436, perplexity: 4.6814
loss: 1.6327, perplexity: 5.1179
loss: 1.6758, perplexity: 5.3429
loss: 1.6510, perplexity: 5.2123
loss: 1.6469, perplexity: 5.1910
loss: 1.6272, perplexity: 5.0896
loss: 1.6373, perplexity: 5.1414
loss: 1.6612, perplexity: 5.2655
loss: 1.6561, perplexity: 5.2390
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 31
loss: 1.7191, perplexity: 5.5794
loss: 1.5270, perplexity: 4.6042
loss: 1.5280, perplexity: 4.6090
loss: 1.4736, perplexity: 4.3649
loss: 1.4513, perplexity: 4.2688
loss: 1.4562, perplexity: 4.2895
loss: 1.4548, perplexity: 4.2835
loss: 1.4447, perplexity: 4.2404
loss: 1.4845, perplexity: 4.4129
loss: 1.4998, perplexity: 4.4809
loss: 1.5073, perplexity: 4.5146
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 32
loss: 1.2273, perplexity: 3.4119
loss: 1.2757, perplexity: 3.5811
loss: 1.3015, perplexity: 3.6749
loss: 1.2472, perplexity: 3.4805
loss: 1.3419, perplexity: 3.8264
loss: 1.3834, perplexity: 3.9886
loss: 1.3343, perplexity: 3.7973
loss: 1.3560, perplexity: 3.8805
loss: 1.3364, perplexity: 3.8052
loss: 1.3676, perplexity: 3.9258
loss: 1.3722, perplexity: 3.9441
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 33
loss: 1.1874, perplexity: 3.2785
loss: 1.0893, perplexity: 2.9722
loss: 1.1035, perplexity: 3.0148
loss: 1.1021, perplexity: 3.0104
loss: 1.1143, perplexity: 3.0474
loss: 1.1093, perplexity: 3.0322
loss: 1.1234, perplexity: 3.0752
loss: 1.1382, perplexity: 3.1213
loss: 1.1361, perplexity: 3.1147
loss: 1.1755, perplexity: 3.2396
loss: 1.1806, perplexity: 3.2563
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 34
loss: 1.1528, perplexity: 3.1670
loss: 1.0126, perplexity: 2.7527
loss: 1.0369, perplexity: 2.8206
loss: 1.0754, perplexity: 2.9313
loss: 1.1137, perplexity: 3.0455
loss: 1.1138, perplexity: 3.0459
loss: 1.1180, perplexity: 3.0587
loss: 1.0970, perplexity: 2.9950
loss: 1.0843, perplexity: 2.9574
loss: 1.0770, perplexity: 2.9359
loss: 1.0808, perplexity: 2.9469
>> saved: state_dict/GPT2-Chinese_raw_test_perplexity_2.9469
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 35
loss: 0.9088, perplexity: 2.4813
loss: 0.8391, perplexity: 2.3143
loss: 0.8662, perplexity: 2.3780
loss: 0.9382, perplexity: 2.5553
loss: 0.9526, perplexity: 2.5924
loss: 0.9386, perplexity: 2.5565
loss: 0.9321, perplexity: 2.5398
loss: 0.9107, perplexity: 2.4861
loss: 0.9129, perplexity: 2.4914
loss: 0.8957, perplexity: 2.4490
loss: 0.9037, perplexity: 2.4688
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 36
loss: 0.7425, perplexity: 2.1011
loss: 0.8991, perplexity: 2.4574
loss: 0.8354, perplexity: 2.3058
loss: 0.8191, perplexity: 2.2685
loss: 0.8030, perplexity: 2.2322
loss: 0.8064, perplexity: 2.2399
loss: 0.8002, perplexity: 2.2260
loss: 0.7862, perplexity: 2.1950
loss: 0.7754, perplexity: 2.1716
loss: 0.7659, perplexity: 2.1510
loss: 0.7665, perplexity: 2.1523
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 37
loss: 0.5538, perplexity: 1.7398
loss: 0.6026, perplexity: 1.8269
loss: 0.5858, perplexity: 1.7964
loss: 0.6161, perplexity: 1.8518
loss: 0.6292, perplexity: 1.8760
loss: 0.6165, perplexity: 1.8524
loss: 0.6343, perplexity: 1.8857
loss: 0.6278, perplexity: 1.8735
loss: 0.6294, perplexity: 1.8764
loss: 0.6501, perplexity: 1.9158
loss: 0.6476, perplexity: 1.9110
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 38
loss: 0.4770, perplexity: 1.6112
loss: 0.5189, perplexity: 1.6801
loss: 0.4873, perplexity: 1.6280
loss: 0.5048, perplexity: 1.6567
loss: 0.5352, perplexity: 1.7077
loss: 0.5175, perplexity: 1.6778
loss: 0.5229, perplexity: 1.6868
loss: 0.5397, perplexity: 1.7155
loss: 0.5375, perplexity: 1.7117
loss: 0.5505, perplexity: 1.7340
loss: 0.5552, perplexity: 1.7422
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 39
loss: 0.5444, perplexity: 1.7236
loss: 0.5196, perplexity: 1.6814
loss: 0.5170, perplexity: 1.6770
loss: 0.5098, perplexity: 1.6649
loss: 0.5053, perplexity: 1.6575
loss: 0.4943, perplexity: 1.6394
loss: 0.4883, perplexity: 1.6295
loss: 0.4893, perplexity: 1.6312
loss: 0.5070, perplexity: 1.6604
loss: 0.4988, perplexity: 1.6467
loss: 0.5059, perplexity: 1.6585
>> saved: state_dict/GPT2-Chinese_raw_test_perplexity_1.6585
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 40
loss: 0.4135, perplexity: 1.5121
loss: 0.4375, perplexity: 1.5488
loss: 0.4034, perplexity: 1.4970
loss: 0.4078, perplexity: 1.5035
loss: 0.4001, perplexity: 1.4920
loss: 0.4018, perplexity: 1.4945
loss: 0.3987, perplexity: 1.4899
loss: 0.4072, perplexity: 1.5026
loss: 0.4122, perplexity: 1.5101
loss: 0.4086, perplexity: 1.5048
loss: 0.4107, perplexity: 1.5078
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 41
loss: 0.3681, perplexity: 1.4450
loss: 0.3613, perplexity: 1.4352
loss: 0.3698, perplexity: 1.4475
loss: 0.3651, perplexity: 1.4406
loss: 0.3497, perplexity: 1.4187
loss: 0.3432, perplexity: 1.4095
loss: 0.3345, perplexity: 1.3972
loss: 0.3395, perplexity: 1.4043
loss: 0.3372, perplexity: 1.4010
loss: 0.3347, perplexity: 1.3975
loss: 0.3378, perplexity: 1.4018
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 42
loss: 0.2369, perplexity: 1.2673
loss: 0.2547, perplexity: 1.2901
loss: 0.2730, perplexity: 1.3140
loss: 0.2771, perplexity: 1.3192
loss: 0.2890, perplexity: 1.3351
loss: 0.2788, perplexity: 1.3215
loss: 0.2804, perplexity: 1.3237
loss: 0.2811, perplexity: 1.3246
loss: 0.2897, perplexity: 1.3361
loss: 0.2918, perplexity: 1.3388
loss: 0.2940, perplexity: 1.3418
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 43
loss: 0.2693, perplexity: 1.3091
loss: 0.2606, perplexity: 1.2977
loss: 0.2844, perplexity: 1.3290
loss: 0.2695, perplexity: 1.3094
loss: 0.2629, perplexity: 1.3007
loss: 0.2592, perplexity: 1.2959
loss: 0.2664, perplexity: 1.3052
loss: 0.2659, perplexity: 1.3046
loss: 0.2603, perplexity: 1.2974
loss: 0.2591, perplexity: 1.2957
loss: 0.2588, perplexity: 1.2954
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 44
loss: 0.2366, perplexity: 1.2670
loss: 0.2349, perplexity: 1.2648
loss: 0.2229, perplexity: 1.2498
loss: 0.2103, perplexity: 1.2341
loss: 0.2113, perplexity: 1.2353
loss: 0.2096, perplexity: 1.2332
loss: 0.2096, perplexity: 1.2332
loss: 0.2107, perplexity: 1.2346
loss: 0.2088, perplexity: 1.2322
loss: 0.2110, perplexity: 1.2349
loss: 0.2106, perplexity: 1.2345
>> saved: state_dict/GPT2-Chinese_raw_test_perplexity_1.2345
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 45
loss: 0.1945, perplexity: 1.2147
loss: 0.1804, perplexity: 1.1977
loss: 0.1664, perplexity: 1.1811
loss: 0.1723, perplexity: 1.1881
loss: 0.1781, perplexity: 1.1949
loss: 0.1721, perplexity: 1.1878
loss: 0.1811, perplexity: 1.1985
loss: 0.1800, perplexity: 1.1972
loss: 0.1778, perplexity: 1.1946
loss: 0.1798, perplexity: 1.1970
loss: 0.1805, perplexity: 1.1978
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 46
loss: 0.1195, perplexity: 1.1269
loss: 0.1323, perplexity: 1.1415
loss: 0.1412, perplexity: 1.1517
loss: 0.1395, perplexity: 1.1497
loss: 0.1438, perplexity: 1.1547
loss: 0.1488, perplexity: 1.1605
loss: 0.1502, perplexity: 1.1620
loss: 0.1479, perplexity: 1.1595
loss: 0.1537, perplexity: 1.1661
loss: 0.1535, perplexity: 1.1659
loss: 0.1551, perplexity: 1.1677
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 47
loss: 0.1437, perplexity: 1.1546
loss: 0.1353, perplexity: 1.1449
loss: 0.1384, perplexity: 1.1485
loss: 0.1371, perplexity: 1.1469
loss: 0.1413, perplexity: 1.1518
loss: 0.1417, perplexity: 1.1522
loss: 0.1419, perplexity: 1.1525
loss: 0.1427, perplexity: 1.1534
loss: 0.1452, perplexity: 1.1562
loss: 0.1434, perplexity: 1.1541
loss: 0.1431, perplexity: 1.1539
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 48
loss: 0.1145, perplexity: 1.1213
loss: 0.1093, perplexity: 1.1155
loss: 0.1239, perplexity: 1.1319
loss: 0.1253, perplexity: 1.1335
loss: 0.1269, perplexity: 1.1353
loss: 0.1300, perplexity: 1.1388
loss: 0.1298, perplexity: 1.1386
loss: 0.1304, perplexity: 1.1392
loss: 0.1310, perplexity: 1.1400
loss: 0.1318, perplexity: 1.1409
loss: 0.1334, perplexity: 1.1427
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 49
loss: 0.1050, perplexity: 1.1107
loss: 0.1049, perplexity: 1.1106
loss: 0.1061, perplexity: 1.1120
loss: 0.1079, perplexity: 1.1139
loss: 0.1159, perplexity: 1.1228
loss: 0.1195, perplexity: 1.1269
loss: 0.1194, perplexity: 1.1268
loss: 0.1239, perplexity: 1.1319
loss: 0.1241, perplexity: 1.1321
loss: 0.1246, perplexity: 1.1327
loss: 0.1254, perplexity: 1.1336
>> saved: state_dict/GPT2-Chinese_raw_test_perplexity_1.1336
